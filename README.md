# 长上下文大语言模型如是说

Read this is in [English](./README_en.md).

这是一份对长上下文大语言模型（Long-Context LLM）的全面综述，从架构、框架、训练和评测四个角度展开。由于在arXiv上仍处于on-hold状态，我们将其上传至Github以供大众审阅。

长上下文是自然语言处理（NLP）的重要话题，贯穿NLP架构的发展历程，并为大型语言模型（LLM）提供了巨大的机遇，使LLM具备了类似于人类的终身学习潜力。虽然对更长上下文的追求伴随着诸多障碍，长上下文仍然是当今LLM的核心竞争优势。在过去两年中，LLM的上下文长度实现了突破性的扩展，达到了数百万token，对长上下文LLM的研究重心，也从长度外推，扩展到模型的架构、框架、训练和评测等的方方面面。

受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发，我们将扩展LLM上下文的历程，与人类试图超越自身有限性的努力相类比。在这篇综述中，我们将阐述LLM如何在，对追逐更长上下文，与接受上下文终究有限的事实间挣扎。为实现这一目标，我们将从架构、框架、训练和评测四个角度，全面展示长上下文LLM的生命周期，展现长上下文LLM相关技术的全貌。在本次综述的结尾，我们将提出长上下文LLM目前面临的10个未解决的问题。

本文的结构安排如下。我们希望这篇综述能够作为对长上下文LLM研究的系统性介绍。由于作者知识有限，本综述可能存在遗漏或错误。我们欢迎读者提出建设性的意见。我们会认真考虑这些建议，并在两到三个月后发布修订版。

<p align="center">
    <img src="paper_structure.png" width="50%"> <br>
</p>
