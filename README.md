# 长上下文大语言模型如是说

Read this is in [English](./README_en.md).

这是一份对长上下文大语言模型（Long-Context LLM）的全面综述，从架构、框架、训练和评测四个角度展开。由于在arXiv上仍处于on-hold状态，我们将其上传至Github以供大众审阅。

长上下文是自然语言处理（NLP）的重要话题，贯穿NLP架构的发展历程，并为大型语言模型（LLM）提供了巨大的机遇，使LLM具备了类似于人类的终身学习潜力。虽然对更长上下文的追求伴随着诸多障碍，长上下文仍然是当今LLM的核心竞争优势。在过去两年中，LLM的上下文长度实现了突破性的扩展，达到了数百万token，对长上下文LLM的研究重心，也从长度外推，扩展到模型的架构、框架、训练和评测等的方方面面。

受理查·施特劳斯的交响诗《查拉图斯特拉如是说》的启发，我们将LLM扩展上下文的历程，比作人类试图超越自身有限性的努力。在这篇综述中，我们从**架构**、**框架**、**训练**和**评测**四个角度出发，探讨内容含盖**长度外推**、**高效缓存**、**记忆管理**、**新架构**、**训练框架**、**推理框架**、**长文预训练**、**长文后训练**、**多模态长文（长视频）**、**长文评测**，全面展示长文LLM的生命周期，展现长文LLM相关技术的全貌。在综述的结尾，我们也总结了长上下文LLM目前面临的**10个未解决的问题**。

本文的结构安排如下。我们希望这篇综述可以作为对长文LLM研究的系统性介绍。由于作者知识有限，本文可能存在遗漏或错误。我们欢迎读者提出建设性的意见。我们会认真考虑这些建议，并在两到三个月后发布修订版。

<p align="center">
    <img src="paper_structure.png" width="50%"> <br>
</p>
